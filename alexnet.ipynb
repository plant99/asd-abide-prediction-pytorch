{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/plant99/.local/lib/python3.6/site-packages/ipykernel_launcher.py:191: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/plant99/.local/lib/python3.6/site-packages/ipykernel_launcher.py:192: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/plant99/.local/lib/python3.6/site-packages/ipykernel_launcher.py:193: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/42]\tTime 3.605 (3.605)\tData 0.441 (0.441)\tLoss 6.9082 (6.9082)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Epoch: [0][1/42]\tTime 2.762 (3.183)\tData 0.275 (0.358)\tLoss 6.9050 (6.9066)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Epoch: [0][2/42]\tTime 2.986 (3.117)\tData 0.512 (0.410)\tLoss 6.9035 (6.9056)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Epoch: [0][3/42]\tTime 3.127 (3.120)\tData 0.281 (0.377)\tLoss 6.8988 (6.9039)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Epoch: [0][4/42]\tTime 2.559 (3.008)\tData 0.243 (0.350)\tLoss 6.9019 (6.9035)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Epoch: [0][5/42]\tTime 2.610 (2.942)\tData 0.298 (0.342)\tLoss 6.8876 (6.9008)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Epoch: [0][6/42]\tTime 2.537 (2.884)\tData 0.217 (0.324)\tLoss 6.8918 (6.8995)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Epoch: [0][7/42]\tTime 3.130 (2.915)\tData 0.382 (0.331)\tLoss 6.8785 (6.8969)\tPrec@1 25.000 (3.125)\tPrec@5 35.000 (4.375)\n",
      "Epoch: [0][8/42]\tTime 3.250 (2.952)\tData 0.592 (0.360)\tLoss 6.8742 (6.8944)\tPrec@1 25.000 (5.556)\tPrec@5 45.000 (8.889)\n",
      "Epoch: [0][9/42]\tTime 2.598 (2.916)\tData 0.237 (0.348)\tLoss 6.8637 (6.8913)\tPrec@1 60.000 (11.000)\tPrec@5 70.000 (15.000)\n",
      "Epoch: [0][10/42]\tTime 3.651 (2.983)\tData 0.893 (0.397)\tLoss 6.8589 (6.8884)\tPrec@1 30.000 (12.727)\tPrec@5 70.000 (20.000)\n",
      "Epoch: [0][11/42]\tTime 2.700 (2.960)\tData 0.301 (0.389)\tLoss 6.8494 (6.8851)\tPrec@1 50.000 (15.833)\tPrec@5 95.000 (26.250)\n",
      "Epoch: [0][12/42]\tTime 3.612 (3.010)\tData 1.224 (0.454)\tLoss 6.8357 (6.8813)\tPrec@1 50.000 (18.462)\tPrec@5 100.000 (31.923)\n",
      "Epoch: [0][13/42]\tTime 3.132 (3.019)\tData 0.376 (0.448)\tLoss 6.8294 (6.8776)\tPrec@1 55.000 (21.071)\tPrec@5 100.000 (36.786)\n",
      "Epoch: [0][14/42]\tTime 3.840 (3.073)\tData 0.924 (0.480)\tLoss 6.8182 (6.8737)\tPrec@1 45.000 (22.667)\tPrec@5 100.000 (41.000)\n",
      "Epoch: [0][15/42]\tTime 5.557 (3.229)\tData 1.760 (0.560)\tLoss 6.8069 (6.8695)\tPrec@1 40.000 (23.750)\tPrec@5 100.000 (44.688)\n",
      "Epoch: [0][16/42]\tTime 3.345 (3.235)\tData 0.489 (0.556)\tLoss 6.7859 (6.8646)\tPrec@1 75.000 (26.765)\tPrec@5 100.000 (47.941)\n",
      "Epoch: [0][17/42]\tTime 2.951 (3.220)\tData 0.251 (0.539)\tLoss 6.7801 (6.8599)\tPrec@1 55.000 (28.333)\tPrec@5 100.000 (50.833)\n",
      "Epoch: [0][18/42]\tTime 3.583 (3.239)\tData 1.011 (0.564)\tLoss 6.7680 (6.8550)\tPrec@1 60.000 (30.000)\tPrec@5 100.000 (53.421)\n",
      "Epoch: [0][19/42]\tTime 3.116 (3.233)\tData 0.263 (0.548)\tLoss 6.7576 (6.8502)\tPrec@1 50.000 (31.000)\tPrec@5 100.000 (55.750)\n",
      "Epoch: [0][20/42]\tTime 3.639 (3.252)\tData 0.586 (0.550)\tLoss 6.7355 (6.8447)\tPrec@1 45.000 (31.667)\tPrec@5 100.000 (57.857)\n",
      "Epoch: [0][21/42]\tTime 2.564 (3.221)\tData 0.240 (0.536)\tLoss 6.7141 (6.8388)\tPrec@1 45.000 (32.273)\tPrec@5 100.000 (59.773)\n",
      "Epoch: [0][22/42]\tTime 3.471 (3.232)\tData 0.199 (0.522)\tLoss 6.6981 (6.8326)\tPrec@1 30.000 (32.174)\tPrec@5 100.000 (61.522)\n",
      "Epoch: [0][23/42]\tTime 3.212 (3.231)\tData 0.278 (0.511)\tLoss 6.6757 (6.8261)\tPrec@1 40.000 (32.500)\tPrec@5 100.000 (63.125)\n",
      "Epoch: [0][24/42]\tTime 2.765 (3.212)\tData 0.263 (0.501)\tLoss 6.6359 (6.8185)\tPrec@1 55.000 (33.400)\tPrec@5 100.000 (64.600)\n",
      "Epoch: [0][25/42]\tTime 3.076 (3.207)\tData 0.295 (0.493)\tLoss 6.5989 (6.8101)\tPrec@1 60.000 (34.423)\tPrec@5 100.000 (65.962)\n",
      "Epoch: [0][26/42]\tTime 3.801 (3.229)\tData 1.245 (0.521)\tLoss 6.5469 (6.8003)\tPrec@1 70.000 (35.741)\tPrec@5 100.000 (67.222)\n",
      "Epoch: [0][27/42]\tTime 3.093 (3.224)\tData 0.252 (0.512)\tLoss 6.4995 (6.7896)\tPrec@1 70.000 (36.964)\tPrec@5 100.000 (68.393)\n",
      "Epoch: [0][28/42]\tTime 2.780 (3.209)\tData 0.248 (0.503)\tLoss 6.4565 (6.7781)\tPrec@1 40.000 (37.069)\tPrec@5 100.000 (69.483)\n",
      "Epoch: [0][29/42]\tTime 2.810 (3.195)\tData 0.263 (0.495)\tLoss 6.3429 (6.7636)\tPrec@1 45.000 (37.333)\tPrec@5 100.000 (70.500)\n",
      "Epoch: [0][30/42]\tTime 2.841 (3.184)\tData 0.255 (0.487)\tLoss 6.2328 (6.7465)\tPrec@1 50.000 (37.742)\tPrec@5 100.000 (71.452)\n",
      "Epoch: [0][31/42]\tTime 3.171 (3.184)\tData 0.282 (0.481)\tLoss 6.0373 (6.7243)\tPrec@1 50.000 (38.125)\tPrec@5 100.000 (72.344)\n",
      "Epoch: [0][32/42]\tTime 4.257 (3.216)\tData 0.272 (0.474)\tLoss 5.7631 (6.6952)\tPrec@1 55.000 (38.636)\tPrec@5 100.000 (73.182)\n",
      "Epoch: [0][33/42]\tTime 3.146 (3.214)\tData 0.267 (0.468)\tLoss 5.3551 (6.6557)\tPrec@1 45.000 (38.824)\tPrec@5 100.000 (73.971)\n",
      "Epoch: [0][34/42]\tTime 2.682 (3.199)\tData 0.243 (0.462)\tLoss 4.5438 (6.5954)\tPrec@1 65.000 (39.571)\tPrec@5 100.000 (74.714)\n",
      "Epoch: [0][35/42]\tTime 2.745 (3.186)\tData 0.223 (0.455)\tLoss 3.2769 (6.5032)\tPrec@1 20.000 (39.028)\tPrec@5 100.000 (75.417)\n",
      "Epoch: [0][36/42]\tTime 2.777 (3.175)\tData 0.249 (0.449)\tLoss 1.0106 (6.3548)\tPrec@1 70.000 (39.865)\tPrec@5 100.000 (76.081)\n",
      "Epoch: [0][37/42]\tTime 2.592 (3.160)\tData 0.215 (0.443)\tLoss 0.8881 (6.2109)\tPrec@1 45.000 (40.000)\tPrec@5 100.000 (76.711)\n",
      "Epoch: [0][38/42]\tTime 2.608 (3.146)\tData 0.225 (0.438)\tLoss 0.8296 (6.0729)\tPrec@1 45.000 (40.128)\tPrec@5 100.000 (77.308)\n",
      "Epoch: [0][39/42]\tTime 2.563 (3.131)\tData 0.174 (0.431)\tLoss 0.9685 (5.9453)\tPrec@1 60.000 (40.625)\tPrec@5 100.000 (77.875)\n",
      "Epoch: [0][40/42]\tTime 2.699 (3.121)\tData 0.267 (0.427)\tLoss 2.4004 (5.8589)\tPrec@1 25.000 (40.244)\tPrec@5 100.000 (78.415)\n",
      "Epoch: [0][41/42]\tTime 1.505 (3.082)\tData 0.114 (0.420)\tLoss 11.1819 (5.9103)\tPrec@1 62.500 (40.459)\tPrec@5 100.000 (78.623)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/plant99/.local/lib/python3.6/site-packages/ipykernel_launcher.py:227: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/plant99/.local/lib/python3.6/site-packages/ipykernel_launcher.py:228: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/plant99/.local/lib/python3.6/site-packages/ipykernel_launcher.py:236: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/plant99/.local/lib/python3.6/site-packages/ipykernel_launcher.py:237: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/plant99/.local/lib/python3.6/site-packages/ipykernel_launcher.py:238: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/11]\tTime 1.190 (1.190)\tLoss 17.2486 (17.2486)\tPrec@1 40.000 (40.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [1/11]\tTime 1.106 (1.148)\tLoss 14.3266 (15.7876)\tPrec@1 50.000 (45.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [2/11]\tTime 1.136 (1.144)\tLoss 19.4674 (17.0142)\tPrec@1 35.000 (41.667)\tPrec@5 100.000 (100.000)\n",
      "Test: [3/11]\tTime 1.102 (1.133)\tLoss 14.8829 (16.4814)\tPrec@1 50.000 (43.750)\tPrec@5 100.000 (100.000)\n",
      "Test: [4/11]\tTime 1.111 (1.129)\tLoss 22.4436 (17.6738)\tPrec@1 25.000 (40.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [5/11]\tTime 1.085 (1.121)\tLoss 16.0320 (17.4002)\tPrec@1 45.000 (40.833)\tPrec@5 100.000 (100.000)\n",
      "Test: [6/11]\tTime 1.032 (1.109)\tLoss 16.1337 (17.2193)\tPrec@1 45.000 (41.429)\tPrec@5 100.000 (100.000)\n",
      "Test: [7/11]\tTime 1.163 (1.116)\tLoss 17.8488 (17.2980)\tPrec@1 40.000 (41.250)\tPrec@5 100.000 (100.000)\n",
      "Test: [8/11]\tTime 1.152 (1.120)\tLoss 13.6057 (16.8877)\tPrec@1 55.000 (42.778)\tPrec@5 100.000 (100.000)\n",
      "Test: [9/11]\tTime 1.161 (1.124)\tLoss 11.7418 (16.3731)\tPrec@1 60.000 (44.500)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/11]\tTime 0.591 (1.075)\tLoss 16.8875 (16.3905)\tPrec@1 42.857 (44.444)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 44.444 Prec@5 100.000\n",
      "Epoch: [1][0/42]\tTime 3.350 (3.350)\tData 0.397 (0.397)\tLoss 7.6907 (7.6907)\tPrec@1 75.000 (75.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][1/42]\tTime 4.501 (3.926)\tData 1.617 (1.007)\tLoss 7.4710 (7.5808)\tPrec@1 50.000 (62.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][2/42]\tTime 3.107 (3.653)\tData 0.304 (0.773)\tLoss 1.1981 (5.4533)\tPrec@1 55.000 (60.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][3/42]\tTime 2.785 (3.436)\tData 0.222 (0.635)\tLoss 2.2755 (4.6588)\tPrec@1 50.000 (57.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][4/42]\tTime 2.993 (3.347)\tData 0.242 (0.556)\tLoss 2.0187 (4.1308)\tPrec@1 50.000 (56.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][5/42]\tTime 2.855 (3.265)\tData 0.283 (0.511)\tLoss 4.0259 (4.1133)\tPrec@1 35.000 (52.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][6/42]\tTime 3.016 (3.230)\tData 0.259 (0.475)\tLoss 4.9091 (4.2270)\tPrec@1 35.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][7/42]\tTime 2.867 (3.184)\tData 0.269 (0.449)\tLoss 5.1249 (4.3392)\tPrec@1 50.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][8/42]\tTime 2.619 (3.122)\tData 0.225 (0.424)\tLoss 5.1946 (4.4343)\tPrec@1 60.000 (51.111)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][9/42]\tTime 3.950 (3.204)\tData 1.128 (0.495)\tLoss 5.4411 (4.5350)\tPrec@1 30.000 (49.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][10/42]\tTime 3.231 (3.207)\tData 0.908 (0.532)\tLoss 4.9841 (4.5758)\tPrec@1 55.000 (49.545)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][11/42]\tTime 2.531 (3.151)\tData 0.224 (0.506)\tLoss 4.5854 (4.5766)\tPrec@1 50.000 (49.583)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][12/42]\tTime 2.535 (3.103)\tData 0.242 (0.486)\tLoss 4.0121 (4.5332)\tPrec@1 45.000 (49.231)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][13/42]\tTime 2.553 (3.064)\tData 0.213 (0.467)\tLoss 2.7012 (4.4023)\tPrec@1 50.000 (49.286)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][14/42]\tTime 2.538 (3.029)\tData 0.197 (0.449)\tLoss 3.2131 (4.3230)\tPrec@1 40.000 (48.667)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][15/42]\tTime 2.505 (2.996)\tData 0.215 (0.434)\tLoss 2.7527 (4.2249)\tPrec@1 55.000 (49.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][16/42]\tTime 2.505 (2.967)\tData 0.191 (0.420)\tLoss 2.0443 (4.0966)\tPrec@1 65.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][17/42]\tTime 2.533 (2.943)\tData 0.217 (0.408)\tLoss 2.3207 (3.9980)\tPrec@1 50.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][18/42]\tTime 2.478 (2.919)\tData 0.160 (0.395)\tLoss 1.3885 (3.8606)\tPrec@1 45.000 (49.737)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][19/42]\tTime 2.589 (2.902)\tData 0.220 (0.387)\tLoss 0.7955 (3.7074)\tPrec@1 35.000 (49.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][20/42]\tTime 2.620 (2.889)\tData 0.273 (0.381)\tLoss 0.9665 (3.5768)\tPrec@1 55.000 (49.286)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][21/42]\tTime 2.513 (2.872)\tData 0.217 (0.374)\tLoss 1.9023 (3.5007)\tPrec@1 35.000 (48.636)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][22/42]\tTime 2.575 (2.859)\tData 0.259 (0.369)\tLoss 1.4995 (3.4137)\tPrec@1 55.000 (48.913)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][23/42]\tTime 2.554 (2.846)\tData 0.218 (0.362)\tLoss 1.4447 (3.3317)\tPrec@1 60.000 (49.375)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][24/42]\tTime 2.549 (2.834)\tData 0.247 (0.358)\tLoss 1.8755 (3.2734)\tPrec@1 50.000 (49.400)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][25/42]\tTime 2.541 (2.823)\tData 0.206 (0.352)\tLoss 1.8521 (3.2188)\tPrec@1 50.000 (49.423)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][26/42]\tTime 2.508 (2.811)\tData 0.187 (0.346)\tLoss 1.3760 (3.1505)\tPrec@1 65.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][27/42]\tTime 2.544 (2.802)\tData 0.210 (0.341)\tLoss 1.9243 (3.1067)\tPrec@1 45.000 (49.821)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][28/42]\tTime 2.611 (2.795)\tData 0.255 (0.338)\tLoss 1.8540 (3.0635)\tPrec@1 40.000 (49.483)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][29/42]\tTime 2.552 (2.787)\tData 0.258 (0.335)\tLoss 1.2506 (3.0031)\tPrec@1 60.000 (49.833)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][30/42]\tTime 2.564 (2.780)\tData 0.253 (0.333)\tLoss 0.9377 (2.9365)\tPrec@1 65.000 (50.323)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][31/42]\tTime 2.564 (2.773)\tData 0.208 (0.329)\tLoss 0.9248 (2.8736)\tPrec@1 55.000 (50.469)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][32/42]\tTime 2.562 (2.767)\tData 0.222 (0.326)\tLoss 0.9112 (2.8141)\tPrec@1 40.000 (50.152)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][33/42]\tTime 2.558 (2.760)\tData 0.229 (0.323)\tLoss 0.7079 (2.7522)\tPrec@1 55.000 (50.294)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][34/42]\tTime 2.545 (2.754)\tData 0.215 (0.320)\tLoss 0.7780 (2.6958)\tPrec@1 45.000 (50.143)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][35/42]\tTime 2.587 (2.750)\tData 0.199 (0.316)\tLoss 0.6907 (2.6401)\tPrec@1 65.000 (50.556)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][36/42]\tTime 2.612 (2.746)\tData 0.250 (0.315)\tLoss 1.1324 (2.5993)\tPrec@1 40.000 (50.270)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][37/42]\tTime 2.628 (2.743)\tData 0.290 (0.314)\tLoss 1.2464 (2.5637)\tPrec@1 35.000 (49.868)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][38/42]\tTime 2.676 (2.741)\tData 0.369 (0.315)\tLoss 1.0585 (2.5251)\tPrec@1 40.000 (49.615)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][39/42]\tTime 2.610 (2.738)\tData 0.235 (0.313)\tLoss 0.7636 (2.4811)\tPrec@1 50.000 (49.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][40/42]\tTime 2.507 (2.732)\tData 0.181 (0.310)\tLoss 0.7100 (2.4379)\tPrec@1 50.000 (49.634)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [1][41/42]\tTime 1.344 (2.699)\tData 0.097 (0.305)\tLoss 0.7053 (2.4212)\tPrec@1 50.000 (49.638)\tPrec@5 100.000 (100.000)\n",
      "Test: [0/11]\tTime 1.026 (1.026)\tLoss 0.8185 (0.8185)\tPrec@1 40.000 (40.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [1/11]\tTime 1.104 (1.065)\tLoss 0.7242 (0.7714)\tPrec@1 55.000 (47.500)\tPrec@5 100.000 (100.000)\n",
      "Test: [2/11]\tTime 1.029 (1.053)\tLoss 0.8141 (0.7856)\tPrec@1 40.000 (45.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [3/11]\tTime 1.047 (1.052)\tLoss 0.7059 (0.7657)\tPrec@1 60.000 (48.750)\tPrec@5 100.000 (100.000)\n",
      "Test: [4/11]\tTime 1.034 (1.048)\tLoss 0.8178 (0.7761)\tPrec@1 40.000 (47.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [5/11]\tTime 1.090 (1.055)\tLoss 0.8520 (0.7887)\tPrec@1 35.000 (45.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [6/11]\tTime 1.043 (1.053)\tLoss 0.7589 (0.7845)\tPrec@1 50.000 (45.714)\tPrec@5 100.000 (100.000)\n",
      "Test: [7/11]\tTime 1.079 (1.057)\tLoss 0.7586 (0.7812)\tPrec@1 50.000 (46.250)\tPrec@5 100.000 (100.000)\n",
      "Test: [8/11]\tTime 1.127 (1.064)\tLoss 0.8804 (0.7923)\tPrec@1 30.000 (44.444)\tPrec@5 100.000 (100.000)\n",
      "Test: [9/11]\tTime 1.028 (1.061)\tLoss 0.7630 (0.7893)\tPrec@1 50.000 (45.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/11]\tTime 0.469 (1.007)\tLoss 0.8885 (0.7927)\tPrec@1 28.571 (44.444)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 44.444 Prec@5 100.000\n",
      "Epoch: [2][0/42]\tTime 2.354 (2.354)\tData 0.096 (0.096)\tLoss 0.7081 (0.7081)\tPrec@1 60.000 (60.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][1/42]\tTime 2.491 (2.422)\tData 0.196 (0.146)\tLoss 0.8489 (0.7785)\tPrec@1 35.000 (47.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][2/42]\tTime 2.493 (2.446)\tData 0.088 (0.127)\tLoss 0.6747 (0.7439)\tPrec@1 65.000 (53.333)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][3/42]\tTime 2.413 (2.438)\tData 0.137 (0.129)\tLoss 0.8122 (0.7610)\tPrec@1 45.000 (51.250)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][4/42]\tTime 2.357 (2.422)\tData 0.089 (0.121)\tLoss 0.8960 (0.7880)\tPrec@1 35.000 (48.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][5/42]\tTime 2.408 (2.419)\tData 0.121 (0.121)\tLoss 0.8450 (0.7975)\tPrec@1 35.000 (45.833)\tPrec@5 100.000 (100.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][6/42]\tTime 2.385 (2.415)\tData 0.088 (0.116)\tLoss 0.7362 (0.7887)\tPrec@1 55.000 (47.143)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][7/42]\tTime 2.439 (2.418)\tData 0.088 (0.113)\tLoss 0.7140 (0.7794)\tPrec@1 50.000 (47.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][8/42]\tTime 2.522 (2.429)\tData 0.154 (0.117)\tLoss 0.7225 (0.7731)\tPrec@1 50.000 (47.778)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][9/42]\tTime 2.403 (2.427)\tData 0.123 (0.118)\tLoss 0.7245 (0.7682)\tPrec@1 45.000 (47.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][10/42]\tTime 2.463 (2.430)\tData 0.142 (0.120)\tLoss 0.7167 (0.7635)\tPrec@1 50.000 (47.727)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][11/42]\tTime 2.497 (2.435)\tData 0.133 (0.121)\tLoss 0.7995 (0.7665)\tPrec@1 30.000 (46.250)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][12/42]\tTime 2.444 (2.436)\tData 0.132 (0.122)\tLoss 0.7464 (0.7650)\tPrec@1 40.000 (45.769)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][13/42]\tTime 2.602 (2.448)\tData 0.318 (0.136)\tLoss 0.7036 (0.7606)\tPrec@1 55.000 (46.429)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][14/42]\tTime 2.468 (2.449)\tData 0.145 (0.137)\tLoss 0.7108 (0.7573)\tPrec@1 50.000 (46.667)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][15/42]\tTime 2.523 (2.454)\tData 0.231 (0.143)\tLoss 0.7004 (0.7537)\tPrec@1 45.000 (46.562)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][16/42]\tTime 2.569 (2.461)\tData 0.276 (0.150)\tLoss 0.7350 (0.7526)\tPrec@1 35.000 (45.882)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][17/42]\tTime 2.527 (2.464)\tData 0.245 (0.156)\tLoss 0.7358 (0.7517)\tPrec@1 40.000 (45.556)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][18/42]\tTime 2.500 (2.466)\tData 0.210 (0.159)\tLoss 0.7059 (0.7493)\tPrec@1 45.000 (45.526)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][19/42]\tTime 2.502 (2.468)\tData 0.222 (0.162)\tLoss 0.7049 (0.7471)\tPrec@1 50.000 (45.750)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][20/42]\tTime 2.650 (2.477)\tData 0.215 (0.164)\tLoss 0.6861 (0.7442)\tPrec@1 60.000 (46.429)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][21/42]\tTime 2.582 (2.481)\tData 0.243 (0.168)\tLoss 0.6998 (0.7421)\tPrec@1 50.000 (46.591)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][22/42]\tTime 2.533 (2.484)\tData 0.230 (0.171)\tLoss 0.7252 (0.7414)\tPrec@1 45.000 (46.522)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][23/42]\tTime 2.498 (2.484)\tData 0.217 (0.173)\tLoss 0.6767 (0.7387)\tPrec@1 45.000 (46.458)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][24/42]\tTime 2.524 (2.486)\tData 0.240 (0.175)\tLoss 0.6898 (0.7368)\tPrec@1 50.000 (46.600)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][25/42]\tTime 2.465 (2.485)\tData 0.182 (0.176)\tLoss 0.7383 (0.7368)\tPrec@1 45.000 (46.538)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][26/42]\tTime 2.566 (2.488)\tData 0.243 (0.178)\tLoss 0.6814 (0.7348)\tPrec@1 55.000 (46.852)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][27/42]\tTime 2.510 (2.489)\tData 0.220 (0.180)\tLoss 0.6260 (0.7309)\tPrec@1 70.000 (47.679)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][28/42]\tTime 2.551 (2.491)\tData 0.246 (0.182)\tLoss 0.8039 (0.7334)\tPrec@1 40.000 (47.414)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][29/42]\tTime 2.552 (2.493)\tData 0.211 (0.183)\tLoss 0.7297 (0.7333)\tPrec@1 40.000 (47.167)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][30/42]\tTime 2.606 (2.497)\tData 0.277 (0.186)\tLoss 0.6974 (0.7321)\tPrec@1 50.000 (47.258)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][31/42]\tTime 2.939 (2.511)\tData 0.530 (0.197)\tLoss 0.7072 (0.7313)\tPrec@1 50.000 (47.344)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][32/42]\tTime 2.872 (2.521)\tData 0.475 (0.205)\tLoss 0.7170 (0.7309)\tPrec@1 40.000 (47.121)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][33/42]\tTime 2.809 (2.530)\tData 0.504 (0.214)\tLoss 0.6881 (0.7296)\tPrec@1 55.000 (47.353)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][34/42]\tTime 2.786 (2.537)\tData 0.231 (0.214)\tLoss 0.7014 (0.7288)\tPrec@1 60.000 (47.714)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][35/42]\tTime 2.646 (2.540)\tData 0.228 (0.215)\tLoss 0.6547 (0.7268)\tPrec@1 65.000 (48.194)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][36/42]\tTime 2.929 (2.551)\tData 0.250 (0.216)\tLoss 0.6591 (0.7249)\tPrec@1 60.000 (48.514)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][37/42]\tTime 3.283 (2.570)\tData 0.312 (0.218)\tLoss 1.0330 (0.7331)\tPrec@1 25.000 (47.895)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][38/42]\tTime 2.611 (2.571)\tData 0.255 (0.219)\tLoss 0.6769 (0.7316)\tPrec@1 60.000 (48.205)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][39/42]\tTime 2.621 (2.572)\tData 0.238 (0.220)\tLoss 0.7676 (0.7325)\tPrec@1 45.000 (48.125)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][40/42]\tTime 2.612 (2.573)\tData 0.282 (0.221)\tLoss 0.7396 (0.7327)\tPrec@1 35.000 (47.805)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [2][41/42]\tTime 1.361 (2.544)\tData 0.108 (0.218)\tLoss 0.7031 (0.7324)\tPrec@1 37.500 (47.705)\tPrec@5 100.000 (100.000)\n",
      "Test: [0/11]\tTime 1.055 (1.055)\tLoss 0.6732 (0.6732)\tPrec@1 65.000 (65.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [1/11]\tTime 1.074 (1.065)\tLoss 0.7007 (0.6869)\tPrec@1 50.000 (57.500)\tPrec@5 100.000 (100.000)\n",
      "Test: [2/11]\tTime 1.080 (1.070)\tLoss 0.6820 (0.6853)\tPrec@1 60.000 (58.333)\tPrec@5 100.000 (100.000)\n",
      "Test: [3/11]\tTime 1.072 (1.070)\tLoss 0.6921 (0.6870)\tPrec@1 55.000 (57.500)\tPrec@5 100.000 (100.000)\n",
      "Test: [4/11]\tTime 1.054 (1.067)\tLoss 0.6724 (0.6841)\tPrec@1 65.000 (59.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [5/11]\tTime 1.028 (1.061)\tLoss 0.7223 (0.6904)\tPrec@1 40.000 (55.833)\tPrec@5 100.000 (100.000)\n",
      "Test: [6/11]\tTime 1.068 (1.062)\tLoss 0.6716 (0.6878)\tPrec@1 65.000 (57.143)\tPrec@5 100.000 (100.000)\n",
      "Test: [7/11]\tTime 1.054 (1.061)\tLoss 0.6921 (0.6883)\tPrec@1 55.000 (56.875)\tPrec@5 100.000 (100.000)\n",
      "Test: [8/11]\tTime 1.080 (1.063)\tLoss 0.7116 (0.6909)\tPrec@1 45.000 (55.556)\tPrec@5 100.000 (100.000)\n",
      "Test: [9/11]\tTime 1.091 (1.066)\tLoss 0.6918 (0.6910)\tPrec@1 55.000 (55.500)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/11]\tTime 0.450 (1.010)\tLoss 0.6908 (0.6910)\tPrec@1 57.143 (55.556)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 55.556 Prec@5 100.000\n",
      "Epoch: [3][0/42]\tTime 2.442 (2.442)\tData 0.088 (0.088)\tLoss 0.8090 (0.8090)\tPrec@1 30.000 (30.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][1/42]\tTime 3.545 (2.994)\tData 1.255 (0.671)\tLoss 0.6813 (0.7451)\tPrec@1 60.000 (45.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][2/42]\tTime 2.420 (2.802)\tData 0.123 (0.489)\tLoss 0.6414 (0.7106)\tPrec@1 65.000 (51.667)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][3/42]\tTime 2.466 (2.718)\tData 0.133 (0.400)\tLoss 0.6682 (0.7000)\tPrec@1 60.000 (53.750)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][4/42]\tTime 2.546 (2.684)\tData 0.158 (0.351)\tLoss 0.7713 (0.7142)\tPrec@1 45.000 (52.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][5/42]\tTime 2.410 (2.638)\tData 0.133 (0.315)\tLoss 0.7689 (0.7234)\tPrec@1 45.000 (50.833)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][6/42]\tTime 2.509 (2.620)\tData 0.129 (0.288)\tLoss 0.7435 (0.7262)\tPrec@1 45.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][7/42]\tTime 2.469 (2.601)\tData 0.151 (0.271)\tLoss 0.7013 (0.7231)\tPrec@1 55.000 (50.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][8/42]\tTime 2.500 (2.590)\tData 0.196 (0.263)\tLoss 0.7154 (0.7223)\tPrec@1 60.000 (51.667)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][9/42]\tTime 2.445 (2.575)\tData 0.139 (0.250)\tLoss 0.7294 (0.7230)\tPrec@1 55.000 (52.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][10/42]\tTime 2.414 (2.561)\tData 0.115 (0.238)\tLoss 0.7318 (0.7238)\tPrec@1 50.000 (51.818)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][11/42]\tTime 2.479 (2.554)\tData 0.134 (0.229)\tLoss 0.7236 (0.7238)\tPrec@1 60.000 (52.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][12/42]\tTime 2.484 (2.548)\tData 0.151 (0.223)\tLoss 0.6728 (0.7198)\tPrec@1 60.000 (53.077)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][13/42]\tTime 2.385 (2.537)\tData 0.088 (0.214)\tLoss 0.7593 (0.7227)\tPrec@1 40.000 (52.143)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][14/42]\tTime 2.403 (2.528)\tData 0.113 (0.207)\tLoss 0.6898 (0.7205)\tPrec@1 55.000 (52.333)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][15/42]\tTime 2.394 (2.519)\tData 0.102 (0.200)\tLoss 0.7111 (0.7199)\tPrec@1 55.000 (52.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][16/42]\tTime 2.429 (2.514)\tData 0.130 (0.196)\tLoss 0.7321 (0.7206)\tPrec@1 50.000 (52.353)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][17/42]\tTime 2.501 (2.513)\tData 0.188 (0.196)\tLoss 0.7462 (0.7220)\tPrec@1 35.000 (51.389)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][18/42]\tTime 2.428 (2.509)\tData 0.136 (0.193)\tLoss 0.7177 (0.7218)\tPrec@1 35.000 (50.526)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][19/42]\tTime 2.500 (2.509)\tData 0.151 (0.191)\tLoss 0.6938 (0.7204)\tPrec@1 55.000 (50.750)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][20/42]\tTime 2.470 (2.507)\tData 0.128 (0.188)\tLoss 0.7180 (0.7203)\tPrec@1 45.000 (50.476)\tPrec@5 100.000 (100.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][21/42]\tTime 2.452 (2.504)\tData 0.108 (0.184)\tLoss 0.7377 (0.7211)\tPrec@1 45.000 (50.227)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][22/42]\tTime 2.482 (2.503)\tData 0.173 (0.184)\tLoss 0.7034 (0.7203)\tPrec@1 55.000 (50.435)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][23/42]\tTime 2.399 (2.499)\tData 0.114 (0.181)\tLoss 0.6633 (0.7179)\tPrec@1 60.000 (50.833)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][24/42]\tTime 2.426 (2.496)\tData 0.138 (0.179)\tLoss 0.6806 (0.7164)\tPrec@1 55.000 (51.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][25/42]\tTime 2.543 (2.498)\tData 0.167 (0.178)\tLoss 0.6647 (0.7145)\tPrec@1 60.000 (51.346)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][26/42]\tTime 2.490 (2.498)\tData 0.180 (0.178)\tLoss 0.6969 (0.7138)\tPrec@1 50.000 (51.296)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][27/42]\tTime 2.450 (2.496)\tData 0.147 (0.177)\tLoss 0.6805 (0.7126)\tPrec@1 50.000 (51.250)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][28/42]\tTime 2.471 (2.495)\tData 0.124 (0.176)\tLoss 0.7088 (0.7125)\tPrec@1 55.000 (51.379)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][29/42]\tTime 2.449 (2.493)\tData 0.144 (0.174)\tLoss 0.7460 (0.7136)\tPrec@1 20.000 (50.333)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][30/42]\tTime 2.475 (2.493)\tData 0.151 (0.174)\tLoss 0.7290 (0.7141)\tPrec@1 40.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][31/42]\tTime 2.542 (2.494)\tData 0.177 (0.174)\tLoss 0.7033 (0.7138)\tPrec@1 60.000 (50.312)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][32/42]\tTime 2.498 (2.494)\tData 0.163 (0.173)\tLoss 0.7254 (0.7141)\tPrec@1 55.000 (50.455)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][33/42]\tTime 2.435 (2.493)\tData 0.146 (0.173)\tLoss 0.8630 (0.7185)\tPrec@1 35.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][34/42]\tTime 2.577 (2.495)\tData 0.180 (0.173)\tLoss 0.8013 (0.7209)\tPrec@1 50.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][35/42]\tTime 2.575 (2.497)\tData 0.208 (0.174)\tLoss 0.7508 (0.7217)\tPrec@1 50.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][36/42]\tTime 2.482 (2.497)\tData 0.148 (0.173)\tLoss 0.5567 (0.7172)\tPrec@1 80.000 (50.811)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][37/42]\tTime 2.876 (2.507)\tData 0.194 (0.174)\tLoss 0.6910 (0.7165)\tPrec@1 60.000 (51.053)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][38/42]\tTime 2.868 (2.516)\tData 0.166 (0.173)\tLoss 0.6726 (0.7154)\tPrec@1 65.000 (51.410)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][39/42]\tTime 3.148 (2.532)\tData 0.385 (0.179)\tLoss 0.6233 (0.7131)\tPrec@1 75.000 (52.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][40/42]\tTime 3.293 (2.550)\tData 0.473 (0.186)\tLoss 0.6526 (0.7116)\tPrec@1 70.000 (52.439)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [3][41/42]\tTime 1.453 (2.524)\tData 0.095 (0.184)\tLoss 0.8232 (0.7127)\tPrec@1 37.500 (52.295)\tPrec@5 100.000 (100.000)\n",
      "Test: [0/11]\tTime 1.252 (1.252)\tLoss 0.9201 (0.9201)\tPrec@1 30.000 (30.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [1/11]\tTime 1.068 (1.160)\tLoss 0.8419 (0.8810)\tPrec@1 40.000 (35.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [2/11]\tTime 1.077 (1.132)\tLoss 0.8057 (0.8559)\tPrec@1 45.000 (38.333)\tPrec@5 100.000 (100.000)\n",
      "Test: [3/11]\tTime 1.079 (1.119)\tLoss 0.6941 (0.8155)\tPrec@1 60.000 (43.750)\tPrec@5 100.000 (100.000)\n",
      "Test: [4/11]\tTime 0.976 (1.090)\tLoss 0.8916 (0.8307)\tPrec@1 35.000 (42.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [5/11]\tTime 1.058 (1.085)\tLoss 0.7645 (0.8197)\tPrec@1 50.000 (43.333)\tPrec@5 100.000 (100.000)\n",
      "Test: [6/11]\tTime 1.115 (1.089)\tLoss 0.8452 (0.8233)\tPrec@1 40.000 (42.857)\tPrec@5 100.000 (100.000)\n",
      "Test: [7/11]\tTime 1.131 (1.094)\tLoss 0.8846 (0.8310)\tPrec@1 35.000 (41.875)\tPrec@5 100.000 (100.000)\n",
      "Test: [8/11]\tTime 1.105 (1.096)\tLoss 0.7705 (0.8243)\tPrec@1 50.000 (42.778)\tPrec@5 100.000 (100.000)\n",
      "Test: [9/11]\tTime 1.066 (1.093)\tLoss 0.7251 (0.8143)\tPrec@1 55.000 (44.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/11]\tTime 0.398 (1.030)\tLoss 0.7159 (0.8110)\tPrec@1 57.143 (44.444)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 44.444 Prec@5 100.000\n",
      "Epoch: [4][0/42]\tTime 2.602 (2.602)\tData 0.121 (0.121)\tLoss 0.7813 (0.7813)\tPrec@1 35.000 (35.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][1/42]\tTime 3.276 (2.939)\tData 0.872 (0.496)\tLoss 0.6855 (0.7334)\tPrec@1 60.000 (47.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][2/42]\tTime 2.507 (2.795)\tData 0.144 (0.379)\tLoss 0.6430 (0.7033)\tPrec@1 65.000 (53.333)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][3/42]\tTime 2.839 (2.806)\tData 0.131 (0.317)\tLoss 0.7029 (0.7032)\tPrec@1 50.000 (52.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][4/42]\tTime 2.670 (2.779)\tData 0.149 (0.283)\tLoss 0.7031 (0.7032)\tPrec@1 45.000 (51.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][5/42]\tTime 2.807 (2.784)\tData 0.157 (0.262)\tLoss 0.6903 (0.7010)\tPrec@1 50.000 (50.833)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][6/42]\tTime 3.530 (2.890)\tData 0.839 (0.345)\tLoss 0.6541 (0.6943)\tPrec@1 60.000 (52.143)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][7/42]\tTime 2.992 (2.903)\tData 0.193 (0.326)\tLoss 0.7645 (0.7031)\tPrec@1 35.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][8/42]\tTime 3.657 (2.987)\tData 0.680 (0.365)\tLoss 0.7360 (0.7068)\tPrec@1 45.000 (49.444)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][9/42]\tTime 4.048 (3.093)\tData 0.631 (0.392)\tLoss 0.7547 (0.7115)\tPrec@1 25.000 (47.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][10/42]\tTime 3.563 (3.136)\tData 0.214 (0.375)\tLoss 0.7070 (0.7111)\tPrec@1 55.000 (47.727)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][11/42]\tTime 3.386 (3.157)\tData 0.227 (0.363)\tLoss 0.6866 (0.7091)\tPrec@1 65.000 (49.167)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][12/42]\tTime 3.001 (3.145)\tData 0.167 (0.348)\tLoss 0.6908 (0.7077)\tPrec@1 45.000 (48.846)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][13/42]\tTime 3.023 (3.136)\tData 0.138 (0.333)\tLoss 0.7082 (0.7077)\tPrec@1 30.000 (47.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][14/42]\tTime 3.094 (3.133)\tData 0.313 (0.332)\tLoss 0.6995 (0.7072)\tPrec@1 50.000 (47.667)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][15/42]\tTime 3.779 (3.173)\tData 0.637 (0.351)\tLoss 0.6995 (0.7067)\tPrec@1 50.000 (47.812)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][16/42]\tTime 5.129 (3.289)\tData 1.601 (0.424)\tLoss 0.7037 (0.7065)\tPrec@1 60.000 (48.529)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][17/42]\tTime 3.176 (3.282)\tData 0.278 (0.416)\tLoss 0.7578 (0.7094)\tPrec@1 35.000 (47.778)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][18/42]\tTime 2.915 (3.263)\tData 0.246 (0.407)\tLoss 0.7285 (0.7104)\tPrec@1 55.000 (48.158)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][19/42]\tTime 3.069 (3.253)\tData 0.299 (0.402)\tLoss 0.7095 (0.7103)\tPrec@1 50.000 (48.250)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][20/42]\tTime 3.528 (3.266)\tData 0.688 (0.415)\tLoss 0.7346 (0.7115)\tPrec@1 25.000 (47.143)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][21/42]\tTime 3.489 (3.276)\tData 0.270 (0.409)\tLoss 0.7233 (0.7120)\tPrec@1 35.000 (46.591)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][22/42]\tTime 3.170 (3.272)\tData 0.523 (0.414)\tLoss 0.7310 (0.7128)\tPrec@1 45.000 (46.522)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][23/42]\tTime 3.108 (3.265)\tData 0.270 (0.408)\tLoss 0.6961 (0.7121)\tPrec@1 50.000 (46.667)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][24/42]\tTime 4.159 (3.301)\tData 0.821 (0.424)\tLoss 0.6792 (0.7108)\tPrec@1 60.000 (47.200)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][25/42]\tTime 5.911 (3.401)\tData 1.741 (0.475)\tLoss 0.6742 (0.7094)\tPrec@1 50.000 (47.308)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][26/42]\tTime 4.624 (3.446)\tData 1.181 (0.501)\tLoss 0.7079 (0.7094)\tPrec@1 45.000 (47.222)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][27/42]\tTime 3.260 (3.440)\tData 0.283 (0.493)\tLoss 0.6721 (0.7080)\tPrec@1 65.000 (47.857)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][28/42]\tTime 3.179 (3.431)\tData 0.304 (0.487)\tLoss 0.6320 (0.7054)\tPrec@1 70.000 (48.621)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][29/42]\tTime 3.176 (3.422)\tData 0.254 (0.479)\tLoss 0.6716 (0.7043)\tPrec@1 60.000 (49.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][30/42]\tTime 3.633 (3.429)\tData 0.275 (0.472)\tLoss 0.6723 (0.7033)\tPrec@1 60.000 (49.355)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][31/42]\tTime 3.517 (3.432)\tData 0.208 (0.464)\tLoss 0.9012 (0.7094)\tPrec@1 40.000 (49.062)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][32/42]\tTime 3.636 (3.438)\tData 0.349 (0.461)\tLoss 0.5137 (0.7035)\tPrec@1 80.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][33/42]\tTime 2.934 (3.423)\tData 0.255 (0.455)\tLoss 0.9116 (0.7096)\tPrec@1 35.000 (49.559)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][34/42]\tTime 2.820 (3.406)\tData 0.235 (0.448)\tLoss 0.7323 (0.7103)\tPrec@1 55.000 (49.714)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][35/42]\tTime 3.017 (3.395)\tData 0.276 (0.444)\tLoss 0.7285 (0.7108)\tPrec@1 45.000 (49.583)\tPrec@5 100.000 (100.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][36/42]\tTime 3.072 (3.387)\tData 0.248 (0.438)\tLoss 0.6483 (0.7091)\tPrec@1 75.000 (50.270)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][37/42]\tTime 3.616 (3.393)\tData 0.418 (0.438)\tLoss 0.6912 (0.7086)\tPrec@1 60.000 (50.526)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][38/42]\tTime 4.298 (3.416)\tData 0.296 (0.434)\tLoss 0.7102 (0.7087)\tPrec@1 50.000 (50.513)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][39/42]\tTime 3.777 (3.425)\tData 0.361 (0.432)\tLoss 0.6848 (0.7081)\tPrec@1 55.000 (50.625)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][40/42]\tTime 4.126 (3.442)\tData 0.591 (0.436)\tLoss 0.7731 (0.7096)\tPrec@1 45.000 (50.488)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][41/42]\tTime 1.805 (3.403)\tData 0.113 (0.429)\tLoss 0.7609 (0.7101)\tPrec@1 37.500 (50.362)\tPrec@5 100.000 (100.000)\n",
      "Test: [0/11]\tTime 1.522 (1.522)\tLoss 0.7447 (0.7447)\tPrec@1 35.000 (35.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [1/11]\tTime 1.389 (1.456)\tLoss 0.7099 (0.7273)\tPrec@1 50.000 (42.500)\tPrec@5 100.000 (100.000)\n",
      "Test: [2/11]\tTime 2.763 (1.891)\tLoss 0.7214 (0.7253)\tPrec@1 45.000 (43.333)\tPrec@5 100.000 (100.000)\n",
      "Test: [3/11]\tTime 2.037 (1.928)\tLoss 0.6856 (0.7154)\tPrec@1 60.000 (47.500)\tPrec@5 100.000 (100.000)\n",
      "Test: [4/11]\tTime 1.780 (1.898)\tLoss 0.6834 (0.7090)\tPrec@1 60.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [5/11]\tTime 1.426 (1.819)\tLoss 0.6739 (0.7032)\tPrec@1 65.000 (52.500)\tPrec@5 100.000 (100.000)\n",
      "Test: [6/11]\tTime 1.348 (1.752)\tLoss 0.6958 (0.7021)\tPrec@1 55.000 (52.857)\tPrec@5 100.000 (100.000)\n",
      "Test: [7/11]\tTime 1.262 (1.691)\tLoss 0.7212 (0.7045)\tPrec@1 45.000 (51.875)\tPrec@5 100.000 (100.000)\n",
      "Test: [8/11]\tTime 1.258 (1.643)\tLoss 0.6826 (0.7021)\tPrec@1 60.000 (52.778)\tPrec@5 100.000 (100.000)\n",
      "Test: [9/11]\tTime 1.439 (1.622)\tLoss 0.6609 (0.6979)\tPrec@1 70.000 (54.500)\tPrec@5 100.000 (100.000)\n",
      "Test: [10/11]\tTime 0.570 (1.527)\tLoss 0.6177 (0.6952)\tPrec@1 85.714 (55.556)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 55.556 Prec@5 100.000\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset  # For custom datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "validation_split = .2\n",
    "shuffle_dataset = False\n",
    "random_seed= 4\n",
    "num_epochs = 5\n",
    "num_classes = 2\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.9\n",
    "print_freq = 1\n",
    "best_prec1 = 0\n",
    "\n",
    "dir = '/home/plant99/Documents/code/pro_per/a-t-b/data/'\n",
    "\n",
    "#dataset\n",
    "\n",
    "data = pd.read_csv(dir + 'data.csv')\n",
    "\n",
    "class CustomDatasetFromImages(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            img_path (string): path to the folder where images are\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        # Transforms\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        self.to_tensor = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "        # Read the csv file\n",
    "        self.data_info = pd.read_csv(csv_path, header=None)\n",
    "        # First column contains the image paths\n",
    "        self.image_arr = np.asarray(dir+'/nii-data/Outputs/cpac/filt_global/png-i/' + self.data_info.iloc[:, 6] + '_alff.nii.jpg')\n",
    "        # Second column is the labels\n",
    "        self.label_arr = np.asarray(self.data_info.iloc[:, 7])\n",
    "        # Third column is for an operation indicator\n",
    "        # self.operation_arr = np.asarray(self.data_info.iloc[:, 2])\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.data_info.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        single_image_name = self.image_arr[index]\n",
    "        # Open image\n",
    "        img_as_img = Image.open(single_image_name)\n",
    "\n",
    "        # Check if there is an operation\n",
    "        #some_operation = self.operation_arr[index]\n",
    "        # If there is an operation\n",
    "        #if some_operation:\n",
    "            # Do some operation on image\n",
    "            # ...\n",
    "            # ...\n",
    "        #    pass\n",
    "        # Transform image to tensor\n",
    "        img_as_tensor = self.to_tensor(img_as_img)\n",
    "\n",
    "        # Get label(class) of the image based on the cropped pandas column\n",
    "        single_image_label = self.label_arr[index]\n",
    "\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])\n",
    "#     transformations = transforms.Compose([\n",
    "#             transforms.RandomSizedCrop(224),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.ToTensor(),\n",
    "#             normalize,\n",
    "#         ])\n",
    "\n",
    "dataset = CustomDatasetFromImages(dir + 'data.csv')\n",
    "    \n",
    "# Data loader\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "#                                          batch_size=batch_size, \n",
    "#                                         shuffle=True)\n",
    "\n",
    "#test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                          batch_size=batch_size, \n",
    "#                                          shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "validation_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=validation_sampler)\n",
    "\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "#         self.fc = nn.Linear(51168, num_classes+1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = self.layer1(x)\n",
    "#         out = self.layer2(out)\n",
    "#         out = out.reshape(out.size(0), -1)\n",
    "#         out = self.fc(out)\n",
    "#         return out\n",
    "\n",
    "model = models.alexnet()\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum,\n",
    "                                weight_decay)\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        #target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        #target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(validation_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            #'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best)\n",
    "\n",
    "\n",
    "    \n",
    "     \n",
    "# # Train the model\n",
    "# total_step = len(train_loader)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "#         labels_ = []\n",
    "#         for label in labels:\n",
    "#             labels_.append(int(label))\n",
    "#         labels = torch.LongTensor(labels_)\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if (i+1) % 100 == 0:\n",
    "#             print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "\n",
    "# # Test the model\n",
    "# model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in validation_loader:\n",
    "#         images = images.to(device)\n",
    "#         labels_ = []\n",
    "#         for label in labels:\n",
    "#             labels_.append(int(label))\n",
    "#         labels = torch.LongTensor(labels_)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "#     print('Test Accuracy of the model on the 1036 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# # Save the model checkpoint\n",
    "# #torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
